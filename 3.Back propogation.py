# Обучение нейронной сети - автоматический подбор весов,чтобы результаты были максимально похожие на правильные 
# Обучение с учителем - наблюдение,когда у нас есть уже правильный ответ
# Back Propogation - алгоритм,который базируется на алгоритме градиентного спуска(обучение с учителем)
# После того,как мы запустили сеть,подобрали веса,указали функцию активации и запустили-
#- у нас выходит ответ "y"
# Мы можем рассчитать ошибку по формуле e = d(ответ) - y
# Корректировка весов(самое главное) начинается в обратном направлении
# 1. Мы вычиляем выходной градиент для нейрона --- б_1 = e * f'(v_out_1)
# f'(v_out) - это производная функции активации(вместо x у нас выходное значение нашей сети)
# *Дкая функция"*
# 2. Мы изменяем весовой коля функции активации в нейронных сетях обычно используют : " Гиперболический тангенс" и " Логистичесэфициент по формуле --- w_1 = w_1 - (L * б * f_1)
# L - шаг обучения нейронной сети(выбирается вручную) (0.1,0.01,0.001)
# f_1 - выходное значение нейрона скрытого слоя 
# 3. Мы вычисляем градиент для другого нейрона --- б_2 = б_1 * w_1 * f'(v_out_2)
# Все вроде тоже самое 
# 4. Вычисляем градиент для следующего нейрона(там 'б' будет 2 на каждый потому что связей стало не 1, а 2)
# б_3 = (б_2 * w_2 + б_22 * w_22) * f'(v_out_3)
# 5. Мы провели 1-ю итерацию и после этого берем другие входные даннные и ответы(рандомные) и проделываем тоже самое \
# По итогу мы уменьшаем нашу ошибку

import numpy as np

# Вводим функцию гиперболического тангенса 
def f(x):
    return 2/(1 + np.exp(-x)) - 1

# Вводим функцию для вычисления производной именно гиперболического тангенса 
def df(x):
    return 0.5*(1 + x)*(1 - x)
# Веса для нейронной сети (1,2 слой)
W1 = np.array([[-0.2, 0.3, -0.4], [0.1, -0.3, -0.4]])
W2 = np.array([0.2, 0.3])

# Пропускаем вектор наблюдений через нейронную сеть 
def go_forward(inp):
    sum = np.dot(W1, inp)
    out = np.array([f(x) for x in sum]) # Получаем выходные значения скрытого слоя 

    sum = np.dot(W2, out)
    y = f(sum)                          # Выходные значения для всей нейросети 
    return (y, out)

def train(epoch):
    global W2, W1
    lmd = 0.01          # шаг обучения
    N = 10000           # число итераций при обучении
    count = len(epoch)  # Определяем размер для нашей обучающей выборки 
    for k in range(N):
        x = epoch[np.random.randint(0, count)]  # случайных выбор входного сигнала из обучающей выборки
        y, out = go_forward(x[0:3])             # прямой проход по НС и вычисление выходных значений нейронов
        e = y - x[-1]                           # ошибка
        delta = e*df(y)                         # локальный градиент
        W2[0] = W2[0] - lmd * delta * out[0]    # корректировка веса первой связи
        W2[1] = W2[1] - lmd * delta * out[1]    # корректировка веса второй связи

        delta2 = W2*delta*df(out)               # вектор из 2-х величин локальных градиентов

        # корректировка связей первого слоя
        W1[0, :] = W1[0, :] - np.array(x[0:3]) * delta2[0] * lmd
        W1[1, :] = W1[1, :] - np.array(x[0:3]) * delta2[1] * lmd

# Входные векторы и выходное значение 
epoch = [(-1, -1, -1, -1),
         (-1, -1, 1, 1),
         (-1, 1, -1, -1),
         (-1, 1, 1, 1),
         (1, -1, -1, -1),
         (1, -1, 1, 1),
         (1, 1, -1, -1),
         (1, 1, 1, -1)]

train(epoch)        # запуск обучения сети

# проверка полученных результатов
for x in epoch:
    y, out = go_forward(x[0:3])
    print(f"Выходное значение НС: {y} => {x[-1]}")
