# LSTM конечно классная сеть,но у нее есть проблема с настраиваемыми параметрами(их слишком много) -> сеть будет тренироваться очень долго,
# поэтому этой RNN нашли замену -> GRU(по эффективности они примерно равны)

# У нас только один выход - h_t 
# Как работает?
# 1. Забывание ненужного
# 1.1 Сначала мы перемножаем старый вектор h_t-1 c нашим входом x_t и проводим серез сигмоидалную функцию с весами и bias-ом -> z_t 
# 1.2 После этого мы от 1 минусуем наш получившийся вектор z_t(инверсия)
# 1.3 Мы перемножаем вектор (h_t-1) на (1 - z_t)  
# 2. Запоминание нового 
# 2.1 Мы умножаем h_t-1 на x_t и проводим через сигмоидальную функцию с bias-ом и весами(тоже самое,что и пункт 1.1) -> r_t
# 2.2 Добавляем к 2.1 вектор x_t 
# 2.3 Вычисляем фнкцию гиперболического тангенса,добавляя веса и bias - q_t
# 2.4 Поэлементно умножаем z_t c q_t -> запоминаем новое 
# 3.Окончательно вычисляем вектор h_t 
# 3.1  поэлементно складываем пункты 1 и 2


# Сначала рекомендуеться использовать сеть GRU(быстрее обучается),после этого пробуем LSTM



# Использование GRU(тоже самое,просто вместо LSTM ставим GRU)
model = Sequential()
model.add(Embedding(maxWordsCount, 128, input_length = max_text_len))
model.add(GRU(128, return_sequences=True))
model.add(GRU(64))
model.add(Dense(2, activation='softmax'))
